<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="utf-8">
  <title>Inferencia e Informacion 2025</title>
  <style>
    body {
      background-color: white;
      color: black;
      font-family: "Trebuchet MS", sans-serif;
    }
    a:link {
      color: #FF6600;
    }
    a:visited, a:active {
      color: #0099CC;
    }
    h1 {
      font-size: 3em;
      text-align: center;
    }
    h2 {
      font-size: 1.5em;
      text-align: center;
    }
    table {
      margin: 0 auto;
      border-collapse: collapse;
    }
    table, th, td {
      border: 1px solid black;
      padding: 10px;
    }
    p {
      max-width: 800px;
      margin: 20px auto;
      line-height: 1.6;
    }
    
    ol {
    max-width: 800px;
    margin: 20px auto;
    line-height: 1.6;
    }

    ul {
    max-width: 800px;
    margin: 20px auto;
    line-height: 1.6;
    }

  </style>
</head>

<body>
  <h1><b>Teoría de la Información e Inferencia Estadística</b></h1>

  <p>La teoría de la información fue introducida por Claude Shannon en 1948 con el objetivo de desarrollar mecanismos que permitieran transferir información digital de manera fiable, incluso en presencia de ruido. Esta teoría constituye la base de toda forma de comunicación digital y, en ese sentido, es uno de los pilares fundamentales de la sociedad moderna.</p>

  <p>Como ocurre con muchas teorías matemáticas, el alcance de la teoría de la información ha superado ampliamente su propósito original. En la actualidad, sus ideas son herramientas esenciales en la construcción de métodos de inferencia estadística y constituyen el fundamento de numerosos algoritmos de aprendizaje automático, en particular aquellos basados en redes neuronales como los grandes modelos de lenguaje.</p>

  <p>Este curso será una introducción a los fundamentos matemáticos de la teoría de la información. Estudiaremos los resultados clásicos de Shannon —como la entropía y la capacidad de canal— así como su papel central en la inferencia estadística (por ejemplo, el principio de máxima entropía de Jaynes), y exploraremos algunas aplicaciones relevantes en machine learning. Más específicamente, el curso cubrirá las siguientes ideas</p>

  <ol>
    <li><strong>Fundamentos clásicos de la teoría de la información</strong>
      <ul>
        <li>Entropía, información mutua y divergencia de Kullback-Leibler</li>
        <li>Compresión de datos (teoremas de compresión de fuente)</li>
        <li>Capacidad de canal y el teorema de Shannon para canales con ruido</li>
      </ul>
    </li>

    <li><strong>Inferencia estadística y principio de máxima entropía</strong>
      <ul>
        <li>El principio de Jaynes y su conexión con la teoría bayesiana</li>
        <li>Aplicaciones en estimación de distribuciones y modelos probabilísticos</li>
        <li>Inferencia como problema de codificación óptima</li>
      </ul>
    </li>

    <li><strong>Aprendizaje, compresión y generalización</strong>
      <ul>
        <li>El principio de longitud mínima de descripción (MDL)</li>
        <li>Información mutua como medida de capacidad de aprendizaje</li>
        <li>Límite de generalización y conexión con complejidad de modelos</li>
      </ul>
    </li>
  </ol>

</body>

  <h2>Bibliografía</h2>
  <ul>
    <li><b>[C] (referencia principal)</b> Cover, T. M., &amp; Thomas, J. A. (2006). <i>Elements of Information Theory</i> (2nd ed.). Wiley-Interscience.</li>
    <li><b>[MK]</b> MacKay, D. J. C. (2003). <i>Information Theory, Inference, and Learning Algorithms</i>. Cambridge University Press. <a href="https://www.inference.org.uk/itprnn/book.pdf" target="_blank">[link]</a></li> 
  </ul>


  <h2>Datos generales del Curso</h2>
  <ul>
    <li><b>HORARIO:</b> Lunes y Miercoles 18.00-19.30 en el IESTA (FCEA)</li>
    <li><b>DURACION:</b> 7 semanas, del 18/08/2025 al 05/10/2025.
    <li><b>MODALIDAD:</b>presencial</li> 
    <li><b>Responsable:</b> Mauricio Velasco (CMAT) <a href="https://mauricio-velasco.github.io/webpage/" target="_blank">[página web]</a> e-mail: mvelasco@cmat.edu.uy </li>
    <li><b>Ayudante:</b> Federico Carrasco. e-mail: fcarrasco@cmat.edu.uy</li>
  </ul>

  <h2>Evaluación</h2>
  <ul>

    <li>La evaluación consistirá de dos parciales teórico-prácticos (25\% c/u) y una exposición final (50%).</li> 
    <li>Total de horas de dedicación del estudiante: 90</li>
    <li><b> Requisitos Previos: </b> Cálculo en varias variables, álgebra lineal y al menos un curso de probabilidad. Estudiantes de la Licenciatura en Estadística: 180 créditos de avance en la carrera.</li> 

  </ul>

  </ul>




  <h1>Cronograma detallado del curso y ejercicios sugeridos</h1>
  <table>
    <tr>
      <th>Semana</th>
      <th>Actividad</th>
      <th>Tema</th>
      <th>Referencia o Link</th>
      <th>Info adicional</th>
    </tr>
    <tr>
      <td>1</td>
      <td>[T1]</td>
      <td>Que es la entropía?</td>
      <td>[C, cap. 2,5]</a></td>
    </tr>
    <tr>
      <td>1</td>
      <td>[P1]</td>
      <td>Ejs Semana 1 </td>
      <td>[C]</td>

    </tr>

  </table>
</body>
</html>
